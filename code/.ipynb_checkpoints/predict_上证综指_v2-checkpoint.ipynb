{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2e75f93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-15T14:47:40.365553Z",
     "start_time": "2024-06-15T14:47:31.894496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 0.0404, Val Loss: 0.0120\n",
      "Epoch 1, Train Loss: 0.0324, Val Loss: 0.0142\n",
      "Epoch 2, Train Loss: 0.0322, Val Loss: 0.0132\n",
      "Epoch 3, Train Loss: 0.0323, Val Loss: 0.0098\n",
      "Epoch 4, Train Loss: 0.0321, Val Loss: 0.0113\n",
      "Epoch 5, Train Loss: 0.0320, Val Loss: 0.0136\n",
      "Epoch 6, Train Loss: 0.0321, Val Loss: 0.0166\n",
      "Epoch 7, Train Loss: 0.0324, Val Loss: 0.0137\n",
      "Early stopping at epoch 8\n",
      "2024-06-13\n",
      "Open: 2928.15\n",
      "High: 2976.49\n",
      "Low: 2903.94\n",
      "Close: 2962.81\n",
      "Volume: 168776.06\n",
      "2024-06-14\n",
      "Open: 2931.57\n",
      "High: 2980.02\n",
      "Low: 2906.67\n",
      "Close: 2966.39\n",
      "Volume: 169400.64\n",
      "2024-06-15\n",
      "Open: 0.00\n",
      "High: 0.00\n",
      "Low: 0.00\n",
      "Close: 0.00\n",
      "Volume: 0.00\n",
      "2024-06-16\n",
      "Open: 0.00\n",
      "High: 0.00\n",
      "Low: 0.00\n",
      "Close: 0.00\n",
      "Volume: 0.00\n",
      "2024-06-17\n",
      "Open: 2931.50\n",
      "High: 2979.97\n",
      "Low: 2906.61\n",
      "Close: 2966.28\n",
      "Volume: 169385.19\n",
      "2024-06-18\n",
      "Open: 2931.41\n",
      "High: 2979.89\n",
      "Low: 2906.51\n",
      "Close: 2966.15\n",
      "Volume: 169362.86\n",
      "2024-06-19\n",
      "Open: 2931.33\n",
      "High: 2979.83\n",
      "Low: 2906.42\n",
      "Close: 2966.05\n",
      "Volume: 169342.56\n",
      "2024-06-20\n",
      "Open: 2931.25\n",
      "High: 2979.78\n",
      "Low: 2906.34\n",
      "Close: 2965.96\n",
      "Volume: 169324.59\n",
      "2024-06-21\n",
      "Open: 2931.19\n",
      "High: 2979.74\n",
      "Low: 2906.26\n",
      "Close: 2965.89\n",
      "Volume: 169307.67\n",
      "2024-06-22\n",
      "Open: 0.00\n",
      "High: 0.00\n",
      "Low: 0.00\n",
      "Close: 0.00\n",
      "Volume: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime, timedelta\n",
    "import xgboost as xgb\n",
    "\n",
    "# 获取股票数据\n",
    "ticker = \"000001.SS\"\n",
    "stock = yf.Ticker(ticker)\n",
    "data = stock.history(start=\"2007-01-01\", end=\"2024-06-14\")\n",
    "\n",
    "if data.empty:\n",
    "    raise ValueError(\"No data fetched for the ticker.\")\n",
    "\n",
    "# 过滤数据，只包含交易日\n",
    "data = data[data['Volume'] > 0]\n",
    "data.drop(columns=['Dividends', 'Stock Splits'], inplace=True, errors='ignore')\n",
    "\n",
    "# 确保日期列为字符串格式\n",
    "data.index = data.index.strftime('%Y-%m-%d')\n",
    "\n",
    "# 数据归一化\n",
    "scaler = MinMaxScaler()\n",
    "features = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# 定义时间序列长度\n",
    "time_steps = 10\n",
    "\n",
    "# 创建序列数据\n",
    "def create_sequences(data, time_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X_seq = data.iloc[i:(i + time_steps)][features].values\n",
    "        X.append(X_seq)\n",
    "        y.append(data.iloc[i + time_steps][features].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# 创建训练和测试数据\n",
    "train_data = data.loc[:'2016-12-31']\n",
    "test_data = data.loc['2017-01-01':]\n",
    "\n",
    "X_train, y_train = create_sequences(train_data, time_steps)\n",
    "X_test, y_test = create_sequences(test_data, time_steps)\n",
    "\n",
    "# 转换为张量\n",
    "X_train_tensor = torch.Tensor(X_train)\n",
    "y_train_tensor = torch.Tensor(y_train)\n",
    "X_test_tensor = torch.Tensor(X_test)\n",
    "y_test_tensor = torch.Tensor(y_test)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=64, shuffle=False)\n",
    "\n",
    "# 定义Attention-based CNN-LSTM模型\n",
    "class AttentionCNNLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(AttentionCNNLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(input_size=64, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        lstm_out, _ = self.lstm(x, (h_0, c_0))\n",
    "        attn_output, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        out = self.fc(attn_output[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# 模型实例化\n",
    "model = AttentionCNNLSTM(input_size=5, hidden_size=128, num_layers=3, output_size=5)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "def train_model(model, train_loader, val_loader, epochs, patience):\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "# 训练模型\n",
    "train_model(model, train_loader, test_loader, epochs=50, patience=5)\n",
    "\n",
    "# 加载最佳模型权重\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# 检查日期有效性\n",
    "def is_holiday(date, holidays):\n",
    "    return date in holidays\n",
    "\n",
    "# 列出2024年的假期\n",
    "chinese_holidays_2024 = [\n",
    "    '2024-01-01',  # New Year's Day\n",
    "    '2024-02-10', '2024-02-11', '2024-02-12', '2024-02-13', '2024-02-14',  # Chinese New Year\n",
    "    '2024-04-04', '2024-04-05',  # Qingming Festival\n",
    "    '2024-05-01',  # Labour Day\n",
    "    '2024-06-10',  # Dragon Boat Festival\n",
    "    '2024-09-19', '2024-09-20',  # Mid-Autumn Festival\n",
    "    '2024-10-01', '2024-10-02', '2024-10-03', '2024-10-04', '2024-10-05', '2024-10-06', '2024-10-07',  # National Day\n",
    "]\n",
    "\n",
    "# 找到有效的交易日期\n",
    "def find_valid_date(start_date, data, holidays):\n",
    "    date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    while date.weekday() >= 5 or date.strftime(\"%Y-%m-%d\") in holidays or data.loc[date.strftime(\"%Y-%m-%d\")]['Volume'] == 0:\n",
    "        date -= timedelta(days=1)\n",
    "    return date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# 预测未来数据\n",
    "def predict_future(model, start_date, days):\n",
    "    start_date = find_valid_date(start_date, data, chinese_holidays_2024)\n",
    "    start_date_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    start_index = data.index.get_loc(start_date)\n",
    "\n",
    "    initial_sequence = data.iloc[start_index - time_steps + 1:start_index][features].values\n",
    "    initial_sequence_scaled = scaler.transform(initial_sequence)\n",
    "    initial_sequence_tensor = torch.Tensor(initial_sequence_scaled).unsqueeze(0)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for i in range(days):\n",
    "        current_date = start_date_dt + timedelta(days=i)\n",
    "        if current_date.weekday() >= 5 or is_holiday(current_date.strftime(\"%Y-%m-%d\"), chinese_holidays_2024):\n",
    "            predictions.append([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                output = model(initial_sequence_tensor)\n",
    "            output_unscaled = scaler.inverse_transform(output.squeeze().numpy().reshape(1, -1))\n",
    "            # 确保体积值非负\n",
    "            output_unscaled[0, -1] = max(0, output_unscaled[0, -1])\n",
    "            output_scaled = scaler.transform(output_unscaled)\n",
    "            predictions.append(output_unscaled.squeeze())\n",
    "            initial_sequence = np.vstack((initial_sequence[1:], output_scaled))\n",
    "            initial_sequence_tensor = torch.Tensor(initial_sequence).unsqueeze(0)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# 设置预测开始日期和预测天数\n",
    "start_date = '2024-06-13'\n",
    "prediction_days = 10\n",
    "\n",
    "# 进行预测\n",
    "try:\n",
    "    predictions = predict_future(model, start_date, prediction_days)\n",
    "except ValueError as e:\n",
    "    print(e)\n",
    "\n",
    "# 输出预测结果\n",
    "current_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    current_date_str = (current_date + timedelta(days=i)).strftime(\"%Y-%m-%d\")\n",
    "    print(current_date_str)\n",
    "    for feature, value in zip(features, pred):\n",
    "        print(f\"{feature}: {value:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5bed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
